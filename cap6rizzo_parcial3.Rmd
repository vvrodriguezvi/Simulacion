---
title: "capitulo6_rizzo"
author: "MARIANA BLANDON MEJIA"
date: "2023-06-15"
output: html_document
---

**CAPITULO 6**

1.Replicar todos los ejemplos y ejercicios propuestos en Rizzo (2019) capítulo 7.

Ejercicio 7.1 ESTIMACION  Y ERROR ESTANDAR

```{r}
m <- 1000
g <- numeric(m)
for (i in 1:m) {
x <- rnorm(2)
g[i] <- abs(x[1] - x[2])
}
est <- mean(g)
#DESVIACION DE LA MEDIA MUESTRAL
sqrt(sum((g - mean(g))^2)) / m
```
EJERCICIO 7.2 ESTIMACION MSE(ERROR CUADRATICO MEDIO)

```{r}
n <- 20
m <- 1000
tmean <- numeric(m)
for (i in 1:m) {
x <- sort(rnorm(n))
tmean[i] <- median(x)
}
mse <- mean(tmean^2)
mse
sqrt(sum((tmean - mean(tmean))^2)) / m 

```


The estimate of MSE for the sample median is approximately 0.068 y $\hat{se}(\hat{MSE})= 0.0825$ 

EJERICIO 7.3
````{r}
n <- 20
K <- n/2 - 1
m <- 1000
mse <- matrix(0, n/2, 6)
trimmed.mse <- function(n, m, k, p) {
#MC est of mse for k-level trimmed mean of
#contaminated normal pN(0,1) + (1-p)N(0,100)
tmean <- numeric(m)
for (i in 1:m) {
sigma <- sample(c(1, 10), size = n,
replace = TRUE, prob = c(p, 1-p))
x <- sort(rnorm(n, 0, sigma))
tmean[i] <- sum(x[(k+1):(n-k)]) / (n-2*k)
}
mse.est <- mean(tmean^2)
se.mse <- sqrt(mean((tmean-mean(tmean))^2)) / sqrt(m)
return(c(mse.est, se.mse))
}
for (k in 0:K) {
mse[k+1, 1:2] <- trimmed.mse(n=n, m=m, k=k, p=1.0)
mse[k+1, 3:4] <- trimmed.mse(n=n, m=m, k=k, p=.95)
mse[k+1, 5:6] <- trimmed.mse(n=n, m=m, k=k, p=.9)
}  
```

EJERCICIO 7.4
NIVEL DE CONFIANZA
```{r}
n <- 20
alpha <- .05
x <- rnorm(n, mean=0, sd=2)
UCL <- (n-1) * var(x) / qchisq(alpha, df=n-1)
```


EJERCICIO 7.5 
NIVEL DE CONFIANZA CON MONTECARLO

```{r}
n <- 20
alpha <- .05
UCL <- replicate(1000, expr = {
x <- rnorm(n, mean = 0, sd = 2)
(n-1) * var(x) / qchisq(alpha, df = n-1)
} )
#count the number of intervals that contain sigma^2=4
sum(UCL > 4)
#or compute the mean to get the confidence level
mean(UCL > 4)

```

EJERCICIO 7.6
NIVEL DE CONFIANZA PARA DATOS NO NORMALES
```{r}
n <- 20
alpha <- .05
UCL <- replicate(1000, expr = {
x <- rchisq(n, df = 2)
(n-1) * var(x) / qchisq(alpha, df = n-1)
} )
sum(UCL > 4)

mean(UCL > 4)

```
In this experiment, only 773 or 77.3% of the intervals contained the population
variance, which is far from the 95% coverage under normality.esto se redujo considerablemente si se compara con el anterior donde el supuesto de normalidad se cumplía.


EJERCICIO 7.7 TASA DE ERROR EMPIRICO TIPO I

```{r}
n <- 20
alpha <- .05
mu0 <- 500
sigma <- 100
m <- 10000 #number of replicates
p <- numeric(m) #storage for p-values
for (j in 1:m) {
x <- rnorm(n, mu0, sigma)
ttest <- t.test(x, alternative = "greater", mu = mu0)
p[j] <- ttest$p.value
}
p.hat <- mean(p < alpha)
se.hat <- sqrt(p.hat * (1 - p.hat) / m)
print(c(p.hat, se.hat))
```

The observed Type I error rate in this simulation is 0.0548, and the stan-
dard error of the estimate is approximately 0.0023

EJERCICIO 7.8 TEST DE ASIMETRIA DE LA NORMALIDAD

```{r}
n <- c(10, 20, 30, 50, 100, 500) #sample sizes
cv <- qnorm(.975, 0, sqrt(6/n)) #crit. values for each n
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
#n is a vector of sample sizes
#we are doing length(n) different simulations
p.reject <- numeric(length(n)) #to store sim. results
m <- 10000 #num. repl. each sim.
for (i in 1:length(n)) {
sktests <- numeric(m) #test decisions
for (j in 1:m) {
x <- rnorm(n[i])
#test decision is 1 (reject) or 0
sktests[j] <- as.integer(abs(sk(x)) >= cv[i] )
}
p.reject[i] <- mean(sktests) #proportion rejected
}
p.reject
```

```{r}
#the exact value of the variance [96] (also see [65] or [285]). Repeating the
#simulation with
cv <- qnorm(.975, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
round(cv, 4)

```
These estimates are closer to the nominal level α = 0.05.


EJERICIO 7.9 POTENCIA DE UNA PRUEBA T
```{r}
n <- 20
m <- 1000
mu0 <- 500
sigma <- 100
mu <- c(seq(450, 650, 10)) #alternatives
M <- length(mu)
power <- numeric(M)
for (i in 1:M) {
mu1 <- mu[i]
pvalues <- replicate(m, expr = {
#simulate under alternative mu1
x <- rnorm(n, mean = mu1, sd = sigma)
ttest <- t.test(x,
alternative = "greater", mu = mu0)
ttest$p.value } )
power[i] <- mean(pvalues <= .05)
}
se <- sqrt(power * (1-power) / m)
library(ggplot2)
df <- data.frame(mean=mu, power=power,
upper=power+2*se, lower=power-2*se)
ggplot(df, aes(x=mean, y=power)) +
geom_line() +
geom_vline(xintercept=500, lty=2) +
geom_hline(yintercept=c(0,.05), lty=1:2) +
geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.2, lwd=1.5)
```


JERCICIO 7.10 Potencia de la prueba de asimetría de la normalidad)
```{r}
alpha <- .1
n <- 30
m <- 2500
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
for (j in 1:N) { #for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
sigma <- sample(c(1, 10), replace = TRUE,
size = n, prob = c(1-e, e))
x <- rnorm(n, 0, sigma)
sktests[i] <- as.integer(abs(sk(x)) >= cv)
}
pwr[j] <- mean(sktests)
}
se <- sqrt(pwr * (1-pwr) / m)
library(ggplot2)
df <- data.frame(epsilon=epsilon, power=pwr,
upper=pwr+2*se, lower=pwr-2*se)
ggplot(df, aes(x=epsilon, y=power)) +
geom_line() + labs(x=bquote(epsilon)) +
geom_hline(yintercept=.1, lty=2) +
geom_pointrange(aes(ymin=lower, ymax=upper))
```

For 0 < ε < 1 the empirical power of the test is greater than 0.10 and highest when ε is about 0.15.

EJERCICIO 7.11 Power comparison of tests of normality.


```{r}
# initialize input and output
library(energy)
alpha <- .1
n <- 30
m <- 2500 #try smaller m for a trial run
epsilon <- .1
test1 <- test2 <- test3 <- numeric(m)
#critical value for the skewness test
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
# estimate power
for (j in 1:m) {
e <- epsilon
sigma <- sample(c(1, 10), replace = TRUE,
size = n, prob = c(1-e, e))
x <- rnorm(n, 0, sigma)
test1[j] <- as.integer(abs(sk(x)) >= cv)
test2[j] <- as.integer(

shapiro.test(x)$p.value <= alpha)

test3[j] <- as.integer(

mvnorm.etest(x, R=200)$p.value <= alpha)

}
print(c(epsilon, mean(test1), mean(test2), mean(test3)))
detach(package:energy)
plot(sim[,1], sim[,2], ylim = c(0, 1), type = "l",
xlab = bquote(epsilon), ylab = "power")
lines(sim[,1], sim[,3], lty = 2)
lines(sim[,1], sim[,4], lty = 4)
abline(h = alpha, lty = 3)
legend("topright", 1, c("skewness", "S-W", "energy"),
lty = c(1,2,4), inset = .02)
```

EJERCICIO 7.12 Count Five test statistic.

```{r}
x1 <- rnorm(20, 0, sd = 1)
x2 <- rnorm(20, 0, sd = 1.5)
y <- c(x1, x2)
group <- rep(1:2, each = length(x1))
boxplot(y ~ group, boxwex = .3, xlim = c(.5, 2.5), main = "")
points(group, y)
```

se procede a encontrar los puntos extremos

```{r}
# now identify the extreme points
range(x1)

range(x2)

i <- which(x1 < min(x2))
j <- which(x2 > max(x1))
x1[i]

x2[j]

out1 <- sum(x1 > max(x2)) + sum(x1 < min(x2))
out2 <- sum(x2 > max(x1)) + sum(x2 < min(x1))
max(c(out1, out2))
```

EJERCICIO 7.13 (Count Five test statistic, cont.)

```{r}
maxout <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(max(c(outx, outy)))
}
n1 <- n2 <- 20
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 1000
# generate samples under H0
stat <- replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
maxout(x, y)
})
print(cumsum(table(stat)) / m)
print(quantile(stat, c(.8, .9, .95)))
```

EJERCICIO 7.14 

```{r}
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
n1 <- n2 <- 20
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 10000
tests <- replicate(m, expr = {
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)
count5test(x, y)
} )
alphahat <- mean(tests)
print(alphahat)
```

If the samples are centered by the population mean, we should expect an
empirical Type I error rate of about 0.055.

EJERCICIO 7.15
```{r}
n1 <- 20
n2 <- 30
alphahat <- mean(replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)
count5test(x, y)
}))

```

EJERCICIO 7.16
```{r}
# generate samples under H1 to estimate power
sigma1 <- 1
sigma2 <- 1.5
power <- mean(replicate(m, expr={
x <- rnorm(20, 0, sigma1)
y <- rnorm(20, 0, sigma2)
count5test(x, y)
}))
print(power)
```

**EJERCICIOS PROPUESTOS**
7.1 Estimate the MSE of the level k trimmed means for random samples
of size 20 generated from a standard Cauchy distribution. (The target
parameter θ is the center or median; the expected value does not exist.)
Summarize the estimates of MSE in a table for k = 1, 2, . . . , 9.

```{r}
# Set the number of simulations
n_sim <- 1000

# Set the sample size
n <- 20

# Function to calculate the level k trimmed mean
trimmed_mean <- function(x, k) {
  n <- length(x)
  sort_x <- sort(x)
  trimmed_x <- sort_x[(k + 1):(n - k)]
  mean(trimmed_x)
}

# Function to calculate the MSE
calculate_mse <- function(k) {
  mse <- replicate(n_sim, {
    x <- rcauchy(n)
    theta_hat <- trimmed_mean(x, k)
    (theta_hat - 0)^2  # True parameter is 0 since we're estimating the center
  })
  mean(mse)
}

# Calculate MSE for each level k
mse_table <- data.frame(k = 1:9, MSE = sapply(1:9, calculate_mse))
print(mse_table)
```

7.2 Plot the empirical power curve for the t-test in Example 7.9, changing
the alternative hypothesis to H1 : μ 6= 500, and keeping the significance
level α = 0.05.

```{r}

n <- 20
m <- 1000
mu0 <- 500
sigma <- 100
mu <- c(seq(450, 650, 10)) #alternatives
M <- length(mu)
power <- numeric(M)
for (i in 1:M) {
mu1 <- mu[i]
pvalues <- replicate(m, expr = {
#simulate under alternative mu1
x <- rnorm(n, mean = mu1, sd = sigma)
ttest <- t.test(x,
alternative = "two.sided", mu = mu0)
ttest$p.value } )
power[i] <- mean(pvalues <= .05)
}
se <- sqrt(power * (1-power) / m)
library(ggplot2)
df <- data.frame(mean=mu, power=power,
upper=power+2*se, lower=power-2*se)
ggplot(df, aes(x=mean, y=power)) +
geom_line() +
geom_vline(xintercept=500, lty=2) +
geom_hline(yintercept=c(0,.05), lty=1:2) +
geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.2, lwd=1.5)
```

7.3 Plot the power curves for the t-test in Example 7.9 for sample sizes 10,
20, 30, 40, and 50, but omit the standard error bars. Plot the curves
on the same graph, each in a different color or different line type, and
include a legend. Comment on the relation between power and sample
size.




7.4 Suppose that X1, . . . , Xn are a random sample from a lognormal distri-
bution. Construct a 95% confidence interval for the parameter μ. Use a
Monte Carlo method to obtain an empirical estimate of the confidence
level when data is generated from standard lognormal.

```{r}
# Set the parameters
n <- 30  # Sample size
n_sim <- 1000  # Number of simulations

# Function to perform the simulation and calculate the confidence level
calculate_confidence_level <- function(n, n_sim) {
  true_parameter <- exp(1)  # True parameter value for the lognormal distribution
  z <- qnorm(0.975)  # Critical value for a 95% confidence level
  
  in_interval <- replicate(n_sim, {
    # Generate random sample from standard lognormal distribution
    x <- rlnorm(n)
    
    # Calculate sample mean and sample standard deviation
    xbar <- mean(x)
    s <- sd(x)
    
    # Construct confidence interval
    lower <- xbar - z * (s/sqrt(n))
    upper <- xbar + z * (s/sqrt(n))
    
    # Check if true parameter falls within confidence interval
    true_parameter >= lower && true_parameter <= upper
  })
  
  # Calculate and return the empirical confidence level
  mean(in_interval) * 100
}

# Perform the simulation and calculate the empirical confidence level
empirical_confidence_level <- calculate_confidence_level(n, n_sim)
print(paste("Empirical Confidence Level:", empirical_confidence_level, "%"))
```

7.5 Refer to Example 1.6 (run length encoding). Use simulation to estimate
the probability that the observed maximum run length for the fair coin
flipping experiment is in [9, 11] in a sample size of 1000. Use the results
of your simulation to estimate the standard error of the maximum run
length for this experiment. Suppose that you observed 1000 coin flips
and the maximum run length was 9. Would you suspect that the coin
is unfair? Explain.

7.6 Suppose a 95% symmetric t-interval is applied to estimate a mean,
but the sample data are non-normal. Then the probability that the
confidence interval covers the mean is not necessarily equal to 0.95. Use
a Monte Carlo experiment to estimate the coverage probability of the
t-interval for random samples of χ^2(2) data with sample size n = 20.
Compare your t-interval results with the simulation results in Example
7.4. (The t-interval should be more robust to departures from normality
than the interval for variance.)

````{r}
n<-20
n_sim<-1000
# Function to perform the simulation and calculate the coverage probability
calculate_coverage_probability <- function(n, n_sim) {
  true_mean <- 2  # True mean value for the χ²(2) distribution
  t <- qt(0.975, df = n - 1)  # Critical value for a 95% confidence level and n-1 degrees of freedom
  
  covers_mean <- replicate(n_sim, {
    # Generate random sample from χ²(2) distribution
    x <- rchisq(n, df = 2)
    
    # Calculate sample mean, sample standard deviation, and standard error
    xbar <- mean(x)
    s <- sd(x)
    se <- s / sqrt(n)
    
    # Construct t-interval
    lower <- xbar - t * se
    upper <- xbar + t * se
    
    # Check if true mean falls within confidence interval
    true_mean >= lower && true_mean <= upper
  })
  
  # Calculate and return the coverage probability
  mean(covers_mean) * 100
}

# Perform the simulation and calculate the coverage probability
coverage_probability <- calculate_coverage_probability(n, n_sim)
print(paste("Coverage Probability:", coverage_probability, "%"))
```

7.7 Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness
√

b1 under normality by a Monte Carlo experiment. Compute the stan-
dard error of the estimates from (2.14) using the normal approxima-
tion for the density (with exact variance formula). Compare the esti-
mated quantiles with the quantiles of the large sample approximation
√b1 ≈ N(0, 6/n).

```{r}
library(e1071)
n <- 30  # Sample size
n_sim <- 1000  # Number of simulations

# Function to perform the simulation and calculate the quantiles
calculate_quantiles <- function(n, n_sim) {
  skewness_values <- replicate(n_sim, {
    # Generate random sample from a normal distribution
    x <- rnorm(n)
    
    # Calculate sample skewness
    skewness(x)
  })
  
  # Calculate the empirical quantiles of the skewness values
  quantiles <- quantile(skewness_values, probs = c(0.025, 0.05, 0.95, 0.975))
  
  # Calculate the standard error using the normal approximation
  se <- sqrt(6/n)
  
  return(list(quantiles = quantiles, se = se))
}

# Perform the simulation and calculate the quantiles and standard error
result <- calculate_quantiles(n, n_sim)

# Compare the estimated quantiles with the quantiles of the large sample approximation
quantiles_estimated <- result$quantiles
quantiles_large_sample <- qnorm(c(0.025, 0.05, 0.95, 0.975), mean = 0, sd = result$se)

# Print the estimated quantiles and the quantiles of the large sample approximation
print("Estimated Quantiles:")
print(quantiles_estimated)
print("Quantiles of Large Sample Approximation:")
print(quantiles_large_sample)
```

7.8Estimate the power of the skewness test of normality against symmetric Beta(α, α) distributions
mirar
```{r}
library(e1071)

# Set the parameters
n <- 30  # Sample size
n_sim <- 1000  # Number of simulations
alpha <- 0.05  # Significance level

# Function to perform the simulation and calculate the power
calculate_power <- function(n, n_sim, alpha) {
  power <- replicate(n_sim, {
    # Generate random sample from symmetric Beta(α, α) distribution
    x <- rbeta(n, shape1 = 2, shape2 = 2)
    
    # Calculate sample skewness
    skewness <- skewness(x)
    
    # Perform the skewness test of normality
    test_result <- skew.test(x)
    
    # Check if the null hypothesis is rejected (test has power)
    test_result$p.value <= alpha
  })
  
  # Calculate and return the power
  mean(power) * 100
}

# Perform the simulation and calculate the power
power <- calculate_power(n, n_sim, alpha)
print(paste("Power:", power, "%"))
```

7.9 Refer to Example 7.16. Repeat the simulation, but also compute the
F test of equal variance, at significance level αˆ.= 0.055. Compare the
power of the Count Five test and F test for small, medium, and large
sample sizes. (Recall that the F test is not applicable for non-normal
distributions.)

7.10 Let X be a non-negative random variable with μ = E[X] < ∞. For a
random sample x1, . . . , xn from the distribution of X, the Gini ratio is
defined by

G =
1
2n2μ
Xn
j=1
Xn
i=1
|xi − xj |.

The Gini ratio is applied in economics to measure inequality in income
distribution (see, e.g., [168]). Note that G can be written in terms of
the order statistics x(i) as
G =
1
n2μ
Xn
i=1
(2i − n − 1)x(i)
.

If the mean is unknown, let Gˆ be the statistic G with μ replaced by
x ̄. Estimate by simulation the mean, median and deciles of Gˆ if X is
standard lognormal. Repeat the procedure for the uniform distribution
and Bernoulli(0.1). Also construct density histograms of the replicates
in each case.

```{r}
# Set the parameters
n <- 100  # Sample size
n_sim <- 1000  # Number of simulations

# Function to perform the simulation and calculate statistics
simulate_gini <- function(n, n_sim, dist_name) {
  gini_values <- replicate(n_sim, {
    # Generate random sample from the specified distribution
    if (dist_name == "lognormal") {
      x <- rlnorm(n)
    } else if (dist_name == "uniform") {
      x <- runif(n)
    } else if (dist_name == "bernoulli") {
      x <- rbinom(n, size = 1, prob = 0.1)
    } else {
      stop("Invalid distribution name!")
    }
    
    # Calculate sample mean
    xbar <- mean(x)
    
    # Calculate Gini ratio statistic
    gini <- sum((2 * seq_len(n) - n - 1) * sort(x)) / (n^2 * xbar)
    
    gini
  })
  
  # Calculate statistics
  gini_mean <- mean(gini_values)
  gini_median <- median(gini_values)
  gini_deciles <- quantile(gini_values, probs = seq(0.1, 0.9, by = 0.1))
  
  # Construct density histogram
  hist(gini_values, breaks = "FD", freq = FALSE, main = paste("Density Histogram of", dist_name))
  
  # Return statistics
  list(mean = gini_mean, median = gini_median, deciles = gini_deciles)
}

# Simulate Gini ratio and calculate statistics for the lognormal distribution
lognormal_stats <- simulate_gini(n, n_sim, "lognormal")
print("Lognormal Distribution:")
print(paste("Mean:", lognormal_stats$mean))
print(paste("Median:", lognormal_stats$median))
print("Deciles:")
print(lognormal_stats$deciles)

# Simulate Gini ratio and calculate statistics for the uniform distribution
uniform_stats <- simulate_gini(n, n_sim, "uniform")
print("Uniform Distribution:")
print(paste("Mean:", uniform_stats$mean))
print(paste("Median:", uniform_stats$median))
print("Deciles",uniform_stats$deciles)

```
